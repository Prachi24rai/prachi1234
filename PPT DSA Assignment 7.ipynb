{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjO5hLoygKsJ"
      },
      "outputs": [],
      "source": [
        "Data Pipelining:\n",
        "1. Q: What is the importance of a well-designed data pipeline in machine learning projects?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data pipelining automates data extraction, transformation, validation, and combination, then loads it for further analysis and visualization. The entire\n",
        " pipeline provides speed from one end to the other by eliminating errors and neutralizing bottlenecks or latency.\n",
        "\n",
        " Data analysts and engineers apply pipeline architecture to allow data to improve business intelligence (BI) and analytics, and targeted functionality.\n",
        "  Business intelligence and analytics use data to acquire insight and efficiency in real-time information and trends.\n",
        "\n",
        "Data-enabled functionality covers crucial subjects such as customer journeys, target customer behavior, robotic process automation, and user experiences."
      ],
      "metadata": {
        "id": "AZrqBypuH7MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training and Validation:\n",
        "2. Q: What are the key steps involved in training and validating machine learning models?"
      ],
      "metadata": {
        "id": "YnKVPJIBIsyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Training validations\n",
        "\n",
        "Training validations involve any validation where the model needs to be retrained. Typically, this includes testing different models during a single\n",
        "training job. These validations are performed in the training/evaluation stage of the model’s development, and are often kept as experimentation code that doesn’t\n",
        " make the final cut to production.\n",
        "\n",
        "A few examples of how training validations are used in practice include:\n",
        "\n",
        "Hyperparameter optimisation — Techniques to find the best set of hyperparameters (e.g. Grid Search) are often used, but not validated. Comparing performance\n",
        " of a model that has gone through a hyperparameter optimization with performance of a model containing a fixed set of hyperparameters is a simple validation.\n",
        "  Complexity can be added to this process by testing the effect of tweaking a single hyper param has an expected outcome on model performance.\n",
        "\n",
        "Cross-validation — Running training on different splits of the data can be translated into validations, for example validating that the performance output\n",
        " of each model is within a given range, ensuring that the model generalises well.\n",
        "\n",
        "Feature selection validations — Understanding how important or influential certain features are should also be a continuous process throughout the model’s lifecycle.\n",
        " Examples include removing features from the training set or adding random noise features, to validate the impact this\n",
        "  has on metrics such as performance/feature importance."
      ],
      "metadata": {
        "id": "_0U4VXKbIuuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Deployment:\n",
        "3. Q: How do you ensure seamless deployment of machine learning models in a product environment?"
      ],
      "metadata": {
        "id": "SSH5GGS6JpAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The Process of Deploying Machine Learning Models\n",
        "ML model deployment involves the following steps:\n",
        "\n",
        "Develop, create, and test the model in a training environment: This step requires rigorous training, testing, and optimization of the model to ensure high\n",
        " performance in production. The model training step determines how models perform in production. ML teams must collaborate to optimize, clean, test, and retest model code.\n",
        "\n",
        "Movement of models to deployment environment: After rigorous testing and optimizing model code, the top-performing models undergo preparation for deployment.\n",
        "\n",
        " Models need a deployment environment that contains all the hardware resources and data required to make the model perform optimally. Different deployment environments include:\n",
        "\n",
        "\n",
        "Containers: Most teams use a container deploying environment because containers are reproducible, predictable, and easy to modify and update, making\n",
        " collaboration among engineers easy. Containers encompass all the hardware, configurations and dependencies necessary to deploy the model, improving\n",
        "  consistency among ML teams.\n",
        "\n",
        "\n",
        "Notebooks: Jupyter and AWS Sagemaker are common notebooks used by data scientists for experimentation in the ML lifecycle. However, notebooks present\n",
        " difficulties like reproducibility and testing for teams. To efficiently use notebooks in the production workflow, teams should consider code organization,\n",
        "  reusability, and dependencies, among other factors.\n",
        "\n",
        "\n",
        "In-App environments: This environment works when certain limitations or constraints exist around using data outside the application.\n",
        "Making models available for end users: ML teams must choose how to make models available for their users. Most can be available on demand or deployed to edge devices.\n",
        "\n",
        "Monitoring: The ML lifecycle continues after deployment. Deployed models must undergo constant monitoring to evaluate the performance\n",
        "\n",
        " and accuracy of models over time. Because data is in a continual state of motion and change, model degradation may occur. In this case,\n",
        "  automating the ML workflow to monitor and retrain models constantly helps ensure the longevity of models."
      ],
      "metadata": {
        "id": "QjxUN3iLJp5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Infrastructure Design:\n",
        "4. Q: What factors should be considered when designing the infrastructure for machine learning projects?"
      ],
      "metadata": {
        "id": "pXGEExZRKupO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model selection\n",
        "\n",
        "Machine learning model selection is the process of selecting a well-fitting model. It determines what data is ingested, what tools are used, which components are required, and how components are interlinked.\n",
        "\n",
        "Data ingestion\n",
        "\n",
        "Data ingestion capabilities are at the core of any machine learning infrastructure. These capabilities are needed to collect data for model training, application, and refinement.\n",
        "\n",
        "In terms of tooling, data ingestion requires connections to data sources, processing pipelines, and storage. These tools need to be scalable, flexible, and highly performant. Frequently, extract, load, transform (ELT) pipelines and data lakes are included to meet these needs.\n",
        "\n",
        "Data ingestion tools enable data from a wide range of sources to be aggregated and stored without requiring significant upfront processing. This allows teams to leverage real-time data and to effectively collaborate on the creation of datasets.\n",
        "\n",
        "ML pipelines automation\n",
        "\n",
        "There are numerous tools available that can automate machine learning workflows according to scripts and event triggers. Pipelines are used to process data, train models, perform monitoring tasks, and deploy results. These tools enable teams to focus on higher-level tasks while helping to increase efficiency and ensure the standardization of processes.\n",
        "\n",
        "When developing your infrastructure, you can create toolchains from scratch by individually integrating and orchestrating tools. You can also adopt pre-built or self-contained pipelines, such as ML Flow Pipelines or Apache Airflow. Learn more in our guide about machine learning automation.\n",
        "\n",
        "Visualization and monitoring\n",
        "\n",
        "Machine learning visualization and monitoring are used to gain perspective on how smoothly  workflows are moving, how accurate model training is, and to derive insights from model results. Visualizations can be integrated at any point in machine learning workflows to enable teams to quickly interpret system metrics. Monitoring should be integrated throughout.\n",
        "\n",
        "When incorporating visualization and monitoring into your machine learning infrastructure, you need to ensure that tools ingest data consistently. If solutions do not integrate with all relevant data sources you will not get meaningful insights. Additionally, you need to keep in mind the resources that these tools require. Make sure that you are choosing solutions that work efficiently and do not create resource conflicts with your training or deployment tools.\n",
        "\n",
        "Model testing\n",
        "\n",
        "Testing machine learning models requires integrating tooling between training and deployment phases. This tooling is used to run models against manually labeled datasets to ensure that the results are as expected. Thorough testing requires:\n",
        "\n",
        "Collection and analysis of both qualitative and quantitative data\n",
        "Multiple training runs in identical environments\n",
        "The ability to identify where errors occurred\n",
        "To set up machine learning testing, you need to add monitoring, data analysis, and visualization tools to your infrastructure. You also need to set up automated creation and management of environments. During set up you should perform integration tests to ensure that components are not causing errors in other components or negatively affecting your test results.\n",
        "\n",
        "Deployment\n",
        "\n",
        "Deployment is the final step that you need to account for in your architecture. This step packages your model and makes it available to development teams for integration into services or applications.\n",
        "\n",
        "If you are offering Machine Learning as a Service (MLaaS), it may also mean deploying the model to a production environment. This deployment enables you to take data from and return results to users. Typically, MLaaS involves containerizing models. When models are hosted in containers, you can deliver them as scalable, distributed services regardless of end environment.\n",
        "\n",
        "Inference\n",
        "\n",
        "In the deployment stage, it is important to evaluate deep learning frameworks and select those that best fit your needs for ongoing inference of new data. You will need to select and optimize the framework that meet your performance requirements in production without exhausting your hardware resources. For example, a computer vision model running in a self driving car must perform inference at millisecond speeds, while taking into account the hardware available on board the car.\n",
        "\n",
        "The process of moving models between frameworks, according to production needs, has been made easier in recent years with the development of universal model file formats. These formats enable you to more easily port models between libraries, such as the Open Neural Network eXchange (ONNX).\n",
        "\n",
        "‍"
      ],
      "metadata": {
        "id": "FE3btdAZK5yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Team Building:\n",
        "5. Q: What are the key roles and skills required in a machine learning team?"
      ],
      "metadata": {
        "id": "WgqXsyVbLISr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Skills Needed for Becoming a Machine Learning Engineer\n",
        "\n",
        "\n",
        "1. Applied Mathematics\n",
        "\n",
        "2. Computer Science Fundamentals and Programming\n",
        "\n",
        "3. Machine Learning Algorithms\n",
        "\n",
        "4. Data Modeling and Evaluation\n",
        "\n",
        "\n",
        "5. Neural Networks\n",
        "\n",
        "6. Natural Language Processing\n",
        "\n",
        "7. Communication Skills"
      ],
      "metadata": {
        "id": "MQyIp_j2LMXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cost Optimization:\n",
        "6. Q: How can cost optimization be achieved in machine learning projects?"
      ],
      "metadata": {
        "id": "lfdP9HroSFbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREPARE\n",
        "\n",
        "This step of the ML lifecycle includes storing the data, labeling the data, and processing the data. Cost control in this phase can be accomplished\n",
        " using the following techniques:\n",
        "\n",
        " Data Storage: ML requires extensive data exploration and transformation. Multiple redundant copies of data are quickly generated, which can lead to exponential\n",
        "  growth in storage costs. Therefore, it is essential to establish a cost control strategy at the storage level. Processes can be established to regularly analyze\n",
        "   source data and either remove duplicative data or archive data to lower cost storage based on compliance policies. For example, for data stored in S3, S3 storage\n",
        "    class analysis can be enabled on any group of objects (based on prefix or object tagging) to automatically analyze storage access patterns. This enables\n",
        "     identification and transition of rarely-accessed data to S3 glacier, lowering costs. S3 intelligent storage can also be used to lower costs of data\n",
        "      that has unpredictable usage patterns. It works by monitoring and moving data between a data tier that is optimized for frequent access and another\n",
        "       lower-cost tier that is optimized for infrequent access.\n",
        "\n",
        "Data Labeling. Data labeling is a key process of identifying raw data (such as images, text files, and videos) and adding one or more\n",
        " meaningful and informative labels to provide context so that an ML model can learn from it. This process can be very time consuming and can quickly\n",
        "  increase costs of a project.\n",
        "\n",
        "  Data Wrangling. In ML, a lot of time is spent in identifying, converting, transforming, and validating raw source data into features that can be used\n",
        "   to train models and make predictions. Amazon SageMaker Data Wrangler can be used to reduce this time spent, lowering the costs of the project.\n",
        "    With Data Wrangler, data can be imported from various data sources, and transformed without requiring coding. Once data is prepared, fully\n",
        "    automated ML workflows can be built with Amazon SageMaker Pipelines and saved for reuse in the Amazon SageMaker Feature Store, eliminating the\n",
        "     costs incurred in preparing this data again.\n",
        "\n",
        "\n",
        "Build\n",
        "\n",
        "\n",
        "Notebook Utilization\n",
        "\n",
        "Test code locally.\n",
        "\n",
        "Use Pipe mode (where applicable) to reduce training time\n",
        "\n",
        "Train and Tune\n",
        "\n",
        "Use Spot Instances.\n",
        "\n",
        "Hyperparameter optimization (HPO).\n",
        "\n",
        "CPU vs GPU\n",
        "\n",
        "Distributed Training.\n",
        "\n",
        "Monitor the performance of your training jobs to identify waste\n",
        "\n",
        "Deploy and Manage\n",
        "\n",
        "\n",
        "Endpoint deploymen\n",
        "\n",
        "Multi-model endpoints\n",
        "\n",
        "Auto Scaling\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eFgCdsVOS8AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. Q: How do you balance cost optimization and model performance in machine learning projects?"
      ],
      "metadata": {
        "id": "WUEuoKrkVIJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Let’s consider the situation of the client I mentioned above…\n",
        "\n",
        "Example: Currently it takes X hours for employees to do this task manually, costing $Y. They tend to be correct Z% of the time but find the task frustrating.\n",
        "\n",
        "We have three quantifiable metrics: time, cost and accuracy. There is also the qualitative point indicating the current solution is undesirable to the\n",
        " employees; in some scenarios you may even wish to capture a quantifiable metric for this too.\n",
        "\n",
        "Goal: Create a solution to remove this task from the employees so they can do work they prefer, improving employee satisfaction whilst saving time and consequently money.\n",
        "\n",
        " the business decided that model accuracy was less important than removing unwanted workload and saving time.\n",
        "\n",
        "So to understand if a model is fit for production, you need to consider and balance multiple metrics. The main three I consider are:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Performance\n",
        "\n",
        "This indicates how good the solution is at predicting the correct outcome.\n",
        "\n",
        "The metric itself varies depending on the type of problem. Whichever is chosen for the machine learning model should also be used to calculate the performance baseline.\n",
        "\n",
        "2. Time\n",
        "\n",
        "This is the duration it takes to complete the task.\n",
        "\n",
        "For the baseline, this is how long it takes without a machine learning model; whether that is with an alternative software solution or manually.\n",
        "\n",
        "3. Money\n",
        "\n",
        "This is the monetary impact of the task.\n",
        "\n",
        "For the baseline, this could be related to how much it costs to complete the task or how many sales are made with the current solution.\n",
        "\n",
        "Example Scenarios\n",
        "Regression — Predicting House Prices\n",
        "Let’s assume we own a real estate agency; the company has a lot of stock and wants to explore whether machine learning can aid the process of deciding the\n",
        "asking price of each house.\n",
        "\n",
        "Currently, a human will read the documentation on the property and make an intelligent decision on the value of the house based upon other similar houses recently\n",
        " sold in that area. They will then decide on the offer price based upon their experiences.\n",
        "\n",
        "\n",
        " We decide to focus on building a model that predicts the value of the house. The model’s prediction can then be used by the agent to decide an appropriate offer price.\n",
        "\n",
        "We are expecting the model to find trends and patterns in the features that relate to price. However, we still appreciate that it may miss nuances\n",
        " that a human can capture — for example, the condition of the property — which is why in this scenario we want them to make the final decision.\n",
        "\n",
        " Performance\n",
        "\n",
        "Baseline — How close to the selling price have the previous predictions by agents been?\n",
        "\n",
        "Metrics — Mean Absolute Error (MAE), Root Mean Square Error (RMSE)\n",
        "\n",
        "We should calculate the current performance values — how well an agent on average makes a good prediction — and compare against our best model’s performance.\n",
        "\n",
        "Time\n",
        "\n",
        "Baseline — how long does it take for an agent to make the prediction?\n",
        "\n",
        "If this is currently a time consuming task for our agents, and using a machine learning model makes it significantly quicker, this alone can justify moving forward with our model.\n",
        "\n",
        "Money\n",
        "\n",
        "Baseline — How much does it cost the company to have the agent manually price the house? How much does it cost to be wrong?\n",
        "\n",
        "Ultimately the agency wants to make the most money. If it’s going to cost the company money investing in a machine learning model, they need to understand where they will save money or where they could potentially earn it."
      ],
      "metadata": {
        "id": "7lETvbvBS8FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data Pipelining:\n",
        "8. Q: How would you handle real-time streaming data in a data pipeline for machine learning?"
      ],
      "metadata": {
        "id": "BcUYWczsZGuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "To build a streaming data pipeline, you’ll need a few tools.\n",
        "\n",
        "First, you’ll require an in-memory framework (such as Spark), which handles batch, real-time analytics, and data processing workloads.\n",
        " You’ll also need a streaming platform (Kafka is a popular choice, but there are others on the market) to build the streaming data pipeline.\n",
        "  In addition, you’ll also need a NoSQL database (many people use HBase, but you have a variety of choices available).\n",
        "\n",
        "Before building the streaming data pipeline, you’ll need to transform, cleanse, validate, and write the data to make sure that it’s in the\n",
        "right format and that it’s useful. To build the streaming data pipeline, you’ll initialize the in-memory framework. Then, you’ll initialize the streaming context.\n",
        "\n",
        "Step three is to fetch the data from the streaming platform. Next, you’ll transform the data. The fifth step is to manage the pipeline to ensure everything is\n",
        " working as it’s supposed to.\n",
        "\n",
        "Streaming data pipelines represent a new frontier in business technology, one that allows you to maintain a competitive advantage and analyze large amounts\n",
        " of information in real time. The right tools enable you to build and maintain your streaming data pipeline and assure data accessibility across the enterprise."
      ],
      "metadata": {
        "id": "E2LpANUgZGzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Q: What are the challenges involved in integrating data from multiple sources in a data pipeline, and how would you address them?"
      ],
      "metadata": {
        "id": "vIR3zGeVZj7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data Integration Challenges\n",
        "\n",
        "\n",
        "1. You have disparate data formats and sources\n",
        "2. Your data isn't available where it needs to be\n",
        "3. You have low-quality or outdated data\n",
        "4. You're using the wrong integration software for your needs\n",
        "5. You have too much data\n",
        "\n",
        "How to Create a Top Data Integration Plan\n",
        "\n",
        "\n",
        "1. Clean up your data\n",
        "2. Introduce clear processes for data management\n",
        "3. Back up your data\n",
        "4. Choose the right software to assist you with data integration\n",
        "5. Manage and maintain your data\n"
      ],
      "metadata": {
        "id": "oQSWsg8VZj-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training and Validation:\n",
        "10. Q: How do you ensure the generalization ability of a trained machine learning model?"
      ],
      "metadata": {
        "id": "H4pn264qZkA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Generalization refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.\n",
        "\n",
        "Develop intuition about overfitting.\n",
        "Determine whether a model is good or not.\n",
        "Divide a data set into a training set and a test set."
      ],
      "metadata": {
        "id": "kIfykgxvZkDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. Q: How do you handle imbalanced datasets during model training and validation?"
      ],
      "metadata": {
        "id": "mu5r9yU4ZkFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Choose Proper Evaluation Metric\n",
        "2. Resampling (Oversampling and Undersampling)\n",
        "3. SMOTE (Synthetic Minority Oversampling Technique)\n",
        "4. BalancedBaggingClassifier\n",
        "5. Threshold moving"
      ],
      "metadata": {
        "id": "WzNK330cZkJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Deployment:\n",
        "12. Q: How do you ensure the reliability and scalability of deployed machine learning models?"
      ],
      "metadata": {
        "id": "DCMcrujNkZWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First deployment of the Machine Learning app\n",
        "\n",
        "The first step would be to create a VM instance on Google cloud’s compute engine. We copy the whole project, allow HTTP traffic, and connect our domain name to it.\n",
        " And that's it. The model is up and running and is visible to users through our domain. People are starting to send requests, everything works as expected,\n",
        "  the system remains highly responsive, the model’s accuracy seems to be on good levels.\n",
        "\n",
        "And we are incredibly happy about it.\n",
        "\n",
        "So we have a monolithic app hosted in one virtual machine which requires a bit of manual work to deploy new changes and restart the service.\n",
        " But it’s not bad. After doing that for a few weeks however, some problems are starting to arise.\n",
        "\n",
        "Deployments require too much manual work\n",
        "\n",
        "Dependencies are starting to get out of sync as we add new library versions and new models\n",
        "\n",
        "Debugging is not straightforward.\n",
        "\n",
        "To solve some of those problems, we add a new CI/CD pipeline and we manage to automate things like building, testing, and deployment.\n",
        "\n",
        "Continuous integration (CI) and Continuous deployment (CD) is a set of practices and pipelines that automates the building, testing, and deployment process.\n",
        "\n",
        "Vertical vs horizontal scaling\n",
        "\n",
        "A load balancer is a device that distributes network traffic across multiple machines and ensures that no single server bears too much load,\n",
        " by spreading the traffic. Therefore, it increases the capacity, reliability, and availability of the system.\n",
        "\n",
        " Scaling out\n",
        "The great thing about this architecture is that it can take us a long way down the road. Honestly, it is probably what most of us will ever\n",
        " need. As traffic grows, we can keep adding more and more instances and load balance the traffic between them. We can also have instances in\n",
        "  different geographic regions ( or availability zones as AWS calls them) to minimize the response time in all parts of the world."
      ],
      "metadata": {
        "id": "cXjshLomkZdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. Q: What steps would you take to monitor the performance of deployed machine learning models and detect anomalies?"
      ],
      "metadata": {
        "id": "uI9DFZfkkZgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Digital transformation, digitalization, Industry 4.0, etc….\n",
        "These are all terms you have probably heard or read about before. However, behind all of these buzz words, the main goal is the use of technology\n",
        "and data to increase productivity and efficiency. The connectivity and flow of information and data between devices and sensors allows for an abundance\n",
        "of available data. The key enabler is then being able to use these vast amounts of available data and actually extract useful information, making it\n",
        " possible to reduce costs, optimize capacity, and keep downtime to a minimum. This is where the recent buzz around machine learning and data analytics comes into play."
      ],
      "metadata": {
        "id": "Z9jVCrmWkZkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Infrastructure Design:\n",
        "14. Q: What factors would you consider when designing the infrastructure for machine learning models that require high availability?"
      ],
      "metadata": {
        "id": "67nwxwAukZmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model selection\n",
        "\n",
        "Machine learning model selection is the process of selecting a well-fitting model. It determines what data is ingested, what tools are used, which components are required, and how components are interlinked.\n",
        "\n",
        "Data ingestion\n",
        "\n",
        "Data ingestion capabilities are at the core of any machine learning infrastructure. These capabilities are needed to collect data for model training, application, and refinement.\n",
        "\n",
        "In terms of tooling, data ingestion requires connections to data sources, processing pipelines, and storage. These tools need to be scalable, flexible, and highly performant. Frequently, extract, load, transform (ELT) pipelines and data lakes are included to meet these needs.\n",
        "\n",
        "Data ingestion tools enable data from a wide range of sources to be aggregated and stored without requiring significant upfront processing. This allows teams to leverage real-time data and to effectively collaborate on the creation of datasets.\n",
        "\n",
        "ML pipelines automation\n",
        "\n",
        "There are numerous tools available that can automate machine learning workflows according to scripts and event triggers. Pipelines are used to process data, train models, perform monitoring tasks, and deploy results. These tools enable teams to focus on higher-level tasks while helping to increase efficiency and ensure the standardization of processes.\n",
        "\n",
        "When developing your infrastructure, you can create toolchains from scratch by individually integrating and orchestrating tools. You can also adopt pre-built or self-contained pipelines, such as ML Flow Pipelines or Apache Airflow. Learn more in our guide about machine learning automation.\n",
        "\n",
        "Visualization and monitoring\n",
        "\n",
        "Machine learning visualization and monitoring are used to gain perspective on how smoothly  workflows are moving, how accurate model training is, and to derive insights from model results. Visualizations can be integrated at any point in machine learning workflows to enable teams to quickly interpret system metrics. Monitoring should be integrated throughout.\n",
        "\n",
        "When incorporating visualization and monitoring into your machine learning infrastructure, you need to ensure that tools ingest data consistently. If solutions do not integrate with all relevant data sources you will not get meaningful insights. Additionally, you need to keep in mind the resources that these tools require. Make sure that you are choosing solutions that work efficiently and do not create resource conflicts with your training or deployment tools.\n",
        "\n",
        "Model testing\n",
        "\n",
        "Testing machine learning models requires integrating tooling between training and deployment phases. This tooling is used to run models against manually labeled datasets to ensure that the results are as expected. Thorough testing requires:\n",
        "\n",
        "Collection and analysis of both qualitative and quantitative data\n",
        "Multiple training runs in identical environments\n",
        "The ability to identify where errors occurred\n",
        "To set up machine learning testing, you need to add monitoring, data analysis, and visualization tools to your infrastructure. You also need to set up automated creation and management of environments. During set up you should perform integration tests to ensure that components are not causing errors in other components or negatively affecting your test results.\n",
        "\n",
        "Deployment\n",
        "\n",
        "Deployment is the final step that you need to account for in your architecture. This step packages your model and makes it available to development teams for integration into services or applications.\n",
        "\n",
        "If you are offering Machine Learning as a Service (MLaaS), it may also mean deploying the model to a production environment. This deployment enables you to take data from and return results to users. Typically, MLaaS involves containerizing models. When models are hosted in containers, you can deliver them as scalable, distributed services regardless of end environment.\n",
        "\n",
        "Inference\n",
        "\n",
        "In the deployment stage, it is important to evaluate deep learning frameworks and select those that best fit your needs for ongoing inference of new data. You will need to select and optimize the framework that meet your performance requirements in production without exhausting your hardware resources. For example, a computer vision model running in a self driving car must perform inference at millisecond speeds, while taking into account the hardware available on board the car.\n",
        "\n",
        "The process of moving models between frameworks, according to production needs, has been made easier in recent years with the development of universal model file formats. These formats enable you to more easily port models between libraries, such as the Open Neural Network eXchange (ONNX).\n",
        "\n",
        "‍"
      ],
      "metadata": {
        "id": "srhnpklikZpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. Q: How would you ensure data security and privacy in the infrastructure design for machine learning projects?"
      ],
      "metadata": {
        "id": "-OKBNEqeZkM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Databases containing data about people usually have columns/attributes that — from a privacy standpoint — can be of one of the following types:\n",
        "\n",
        "(a) Personally Identifiable Information (PII) — these are columns which can pretty much directly link to or identify a person (e.g., social security number, email address, etc.).\n",
        "\n",
        "(b) Quasi-Identifiers (QI) — these are columns which may not be useful by themselves but can be combined with other QIs, query results and some external information to identify an individual (e.g., ZIP code, age, gender, etc.).\n",
        "\n",
        "(c) Sensitive Columns — these are attributes that are not PII or QI but constitute data about the person that needs to be protected for various reasons (e.g., salary, HIV diagnosis, live geo-location, etc.).\n",
        "\n",
        "(d) Non-sensitive Columns — these are the remaining attributes that do not fit (a), (b) or (c) above (e.g., country).\n",
        "\n",
        "K-Anonymity is used to provide a guarantee that any arbitrary (QI-based) query on a large dataset will not reveal information that can help narrow\n",
        " a group down below a threshold of ‘k’ individuals. That is, the technique provides an assurance that there will remain an ambiguity of ‘at-least-k’\n",
        " records for anyone mining for privacy sensitive attributes from a dataset using their knowledge of QIs of specific individuals.\n",
        "\n",
        " Attacks on k-anonymity\n",
        " L-Diversity\n",
        " T-Closeness"
      ],
      "metadata": {
        "id": "a_p1wHPiZkQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Team Building:\n",
        "16. Q: How would you foster collaboration and knowledge sharing among team members in a machine learning project?"
      ],
      "metadata": {
        "id": "WGW9qigtp12d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lign your vision and values\n",
        "The first step to foster collaboration and knowledge sharing is to align your vision and values across the organization. You need to communicate\n",
        " clearly and consistently what your mission, goals, and expectations are, and how they relate to each department and location. You also need to create\n",
        " a shared sense of purpose and identity, and celebrate your achievements and successes. By aligning your vision and values, you can create a common ground\n",
        "  and a sense of belonging for your staff, and encourage them to contribute and learn from each other.\n",
        "\n",
        "  Provide training and support\n",
        "\n",
        "  Create opportunities and incentives\n",
        "Build trust and respect\n",
        "\n",
        "Evaluate and improve\n"
      ],
      "metadata": {
        "id": "kOT6Er0Lp15F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. Q: How do you address conflicts or disagreements within a machine learning team?\n"
      ],
      "metadata": {
        "id": "RRBEXED5p17s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Speak to Team Members Individually\n",
        "Start by having an informal one-on-one with each team member involved in the conflict. This way you can hear people's concerns in a safe, confidential setting. In these meetings:\n",
        "\n",
        "Avoid making assumptions and let people open up in their own time.\n",
        "Reassure them that the discussion is confidential.\n",
        "Ask each party the same questions, to remain impartial.\n",
        "2. Bring People Together\n",
        "Once you've got a better understanding of the conflict and everyone's perspectives, it's time to bring the relevant parties together and act as a moderator.\n",
        "\n",
        "Set some ground rules before getting the conversation underway. Encourage team members to listen to one another, respect each other's points of view, and not interrupt or make personal comments. During the conversation:\n",
        "\n",
        "Keep the tone of the conversation calm and non-threatening.\n",
        "Encourage active listening, so people understand where the other person is coming from.\n",
        "Encourage individuals to share ideas. What do they want or need? What would they be prepared to commit to? Have them to brainstorm some solutions.\n",
        "Ask them about situations where they've worked well together in the past. See if they can build on those positive experiences.\n",
        "If the discussion becomes heated, take a break and reconvene when everyone's had a chance to calm down. Be alert for any passive-aggressive behavior.\n",
        "\n",
        "Read our article Managing Emotion in Your Team for more tips on handling heated conversations.\n",
        "\n",
        "3. Ask the Wider Team for Ideas\n",
        "When a conflict affects the whole team, provided it's not sensitive or confidential, you can ask for everyone's perspective.\n",
        "\n",
        "Talking things out helps you and your team to consider different assumptions, beliefs, and decision-making approaches. This can also be a part of creating a \"psychologically safe\" environment, where people feel comfortable sharing ideas and concerns, thus preventing future conflicts.\n",
        "\n",
        "4. Draw up a Plan\n",
        "Ask the parties to detail agreed-on actions for reconciliation. And get each to commit to this strategy. You can draw up a timetable for actions, ticking them off as and when they are achieved. Hold all relevant parties accountable.\n",
        "\n",
        "5. Follow up\n",
        "Ensure that issues have been resolved properly by following up on the situation. For example, people may still feel irritated but not want to drag things out. You can use one-on-ones to prevent old disagreements from resurfacing. And try an anonymous team survey to get feedback and uncover any lingering frustrations."
      ],
      "metadata": {
        "id": "rPe4ncRJp1-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cost Optimization:\n",
        "18. Q: How would you identify areas of cost optimization in a machine learning project?"
      ],
      "metadata": {
        "id": "gjc7UrNDp2Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decisions around cost optimization aren’t always about cost reduction. The key challenge is to maximize business outcomes delivered from IT spend.\n",
        "\n",
        "1. Project and Investments\n",
        "2. Partner Ecosystem\n",
        "3. IT Processes and Ways of Working\n",
        "4. Technology Ecosystem\n",
        "5. Workforce"
      ],
      "metadata": {
        "id": "LOWFSdulp2G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. Q: What techniques or strategies would you suggest for optimizing the cost of cloud infrastructure in a machine learning project?"
      ],
      "metadata": {
        "id": "9s6mqOnjsv13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Review Pricing and Billing Information\n",
        "Cloud vendors provide billing details explaining the cost of cloud services. You can leverage this information to identify high-cost areas and generate savings.\n",
        " Prioritize and analyze high-spend services and workflows. Understanding the cost of the cloud allows you to make informed spending decisions and avoid paying\n",
        " for redundant resources.\n",
        "\n",
        "2. Set Budgets\n",
        "You can control costs by ensuring all parties know the goals and budgets of each project. Never choose an arbitrary number. Instead, facilitate\n",
        "communication between engineering leaders, product leadership, and executives to understand cost requirements.\n",
        "\n",
        "Base requirements according to the planned packaging and delivery of products and features. For example, determine if it is a free trial or enterprise plan.\n",
        " Reference these requirements as tradeoffs during planning and development alongside requirements like speed and resiliency.\n",
        "\n",
        "Set a monthly budget to plan your cloud computing strategy. This budget may differ according to your organization’s needs. An established budget is crucial\n",
        " for planning your overall spending and optimizing costs.\n",
        "\n",
        "\n",
        "\n",
        "3. Identify Unutilized Resources\n",
        "You can easily optimize cloud costs by looking for unattached and unused resources. Administrators and developers sometimes provision a temporary server\n",
        " for a certain task and might forget to de-provision it after the job finishes. Alternatively, an administrator might forget to remove the storage\n",
        "  attached to terminated instances.\n",
        "\n",
        "These practices result in inflated AWS or Azure bills that charge for resources the organization purchased but no longer uses. A cloud cost optimization\n",
        " strategy helps identify unused and unattached resources and remove them to eliminate unnecessary expenses.\n",
        "\n",
        "4. Identify Idle Resources\n",
        "Like the above practice, you can optimize cloud costs by finding and consolidating idle computing resources.\n",
        "\n",
        "Cloud providers charge for idle resources, even if you do not use them. You can optimize costs by identifying and merging these resources to reduce costs.\n",
        " For example, if your CPU utilization is 10% but the provider charges for 100%, you are wasting a significant amount of computing resources.\n",
        "\n",
        "You should find all idle resources and merge them to cut costs. There is no need to save idle resources for events such as busy seasons or traffic spikes.\n",
        "You can leverage cloud features like auto-scaling, load balancing, and on-demand options to scale up capacity as needed.\n",
        "\n",
        "5. Right-Size the Services\n",
        "Right-sizing enables you to analyze computing services and modify them to the most efficient size. However, it is difficult to manually size instances as\n",
        "there are many possible combinations, as well as memory, graphics, database, storage capacity, and throughput options.\n",
        "\n",
        "You can use right-sizing tools to get change recommendations across instance families. It helps reduce cloud costs and optimize cloud usage,\n",
        "helping achieve peak performance from existing resources.\n",
        "\n",
        "\n",
        "\n",
        "Related content: read our guide to cloud savings\n",
        "\n",
        "6. Use Reserved Instances\n",
        "Reserved instances (RIs) are prepaid compute instances that offer significant pricing discounts. When purchasing RIs from a cloud provider,\n",
        " you select an instance type and typically a region or availability zone, and commit to using the instance for a period of 1 or 3 years. In\n",
        "  exchange, most cloud providers offer discounts of up to 75%. Because you pay up front, you must do the research and plan based on\n",
        "   your historic instance usage. AWS also offers Savings Plans programs that offer similar discounts but allow more flexible usage.\n",
        "\n",
        "\n",
        "\n",
        "7. Use Savings Plans\n",
        "The Savings Plans pricing model is flexible and helps you save up to 70% on your AWS usage. This model offers consistently low prices,\n",
        " similar to RIs, based on one-year or three-year commitments.\n",
        "\n",
        "8. Leverage Spot Instances\n",
        "Unlike Reserved Instances, Spot Instances are available for last-minute purchases. AWS auctions its leftover resources at low prices. However,\n",
        "these instances are unreliable because they are not always available and can sell out quickly. Spot Instances are useful for batch jobs or jobs you\n",
        "can terminate immediately, but not critical, time-consuming ones.\n",
        "\n",
        "The table below shows Spot Instance offerings from the leading cloud providers and key service paramteres, such as the lead time for instance shut\n",
        " down and the maximum time a Spot Instance is allowed to run.\n",
        "\n",
        "\n",
        "\n",
        "Related content: read our guide to cloud cost models\n",
        "\n",
        "8. Limit Data Transfer Fees\n",
        "Migrating data to and from a public cloud can be expensive. Cloud vendors often charge data egress fees to shift data from their platforms and\n",
        " sometimes between regions. You can reduce cloud costs by avoiding unnecessary data transfers.\n",
        "\n",
        "Assess your cloud vendor’s transfer fees and adjust the cloud architecture to minimize the necessary data transfers. For example, you can reduce\n",
        "unnecessary transfers by shifting on-premises applications that frequently access cloud data to the cloud.\n",
        "\n",
        "Evaluate the fees of the transfer methods intended to secure and accelerate data transfers between the cloud and private data centers. You can\n",
        "compare the costs of using a dedicated network connection service, like AWS Direct Connect, Google Cloud Interconnect, or Azure ExpressRoute, to\n",
        "the costs of a physical transfer device like AWS Snowball or Azure Data Box.\n",
        "\n",
        "9. Choose a Single or Multi-Cloud Deployment\n",
        "Multi-cloud deployments help you avoid vendor lock-in and increase availability, but they can be expensive. With a single vendor, you can leverage\n",
        "discounts with large-volume purchases. Switching between different cloud platforms can be a hassle and require extra training. Evaluate whether a\n",
        "single-vendor or multi-cloud environment meets your organization’s needs.\n",
        "\n",
        "10. Monitor Cost Anomalies\n",
        "Use the Cost Management console to set budgets, forecast AWS costs, and optimize your overall cloud costs. The console includes a Cost Anomaly\n",
        " Detection feature that monitors usage and costs with machine learning to identify spending anomalies. You can set alerts to notify you when you\n",
        "  approach or exceed expected spending thresholds. Once you analyze an anomaly’s root cause, you can address it to prevent unexpected costs and stick to your planned budget.\n",
        "\n",
        "11. Use Appropriate Storage Options\n",
        "Amazon S3 is a widely-used cloud storage option in the cloud. It is user-friendly, easy to integrate with other AWS or external services, and\n",
        "offers almost unlimited storage. However, AWS offers various storage tiers with significantly different costs, so choosing the right tier is important\n",
        "\n",
        " to avoid overspending. You can use S3-Intelligent Tiering to automatically track your usage patterns and select the best storage tier.\n",
        "\n",
        "12. Optimize Cloud Costs at Each Stage of the SDLC\n",
        "Costs should not be an afterthought considered after building and launching a product. You should implement cost optimization throughout the software development lifecycle (SDLC).\n",
        "\n",
        "Here is how to integrate cloud costs optimization into the SDLC:\n",
        "\n",
        "Planning – justify the budget and use cost data to inform technical debt-related decisions and the product roadmap. This practice helps reduce unexpected spending and rapidly adjust the budget as needed.\n",
        "Deployment and operation – quickly discover unpredicted spending and adjust costs and budgets.\n",
        "Design and build – record the data needed to make cost-effective architecture decisions. This data informs reports on planned spending and learn the costs of sold goods (unit costs).\n",
        "Monitoring – reassess costs by team, feature, and product to report operational expenditures and return on investment (ROI) according to business initiatives.\n",
        "Engineering decisions have an associated cost. Shifting cost optimization left turns each stage into an opportunity to maximize cloud ROI as soon as possible.\n",
        "\n",
        "13. Identify and Minimize Software License Costs\n",
        "Software licensing is a major component of cloud operating costs. Manual license management is challenging, increasing the risk of paying for unused software licenses. You can find various commercial and public Amazon Machine Instances via the AWS Marketplace. License tracking tools can identify idle or unneeded licenses to help you reduce your spending.\n",
        "\n",
        "14. Implement a Cloud Native Design\n",
        "Replace existing cloud systems with more cost-efficient ones to leverage cloud-specific capabilities. For example, you can design a system with auto-scaling to ensure you only pay for the servers you use.\n",
        "\n",
        "The Well-Architected Tool provides best practice recommendations for cloud architectures. You can also leverage AWS’ extensive documentation and expertise to help design your system and reduce costs using cloud native principles. Cloud native design requires specific skills, so you may need guidance to implement it – typically, organizations modify existing cloud infrastructure rather than designing from scratch.\n",
        "\n",
        "Your design should balance performance, cost-optimization, and other considerations based on your organization’s priorities and objectives. For example, a fast DevOps pipeline in the cloud might not reduce costs.\n",
        "\n",
        "15. Track Cost Center Spending\n",
        "Different teams may individually manage their cloud budgets, so they need a way to track their spending. One way to do this is for each cost center to have a separate AWS account for easy reporting. If multiple teams with different budgets share one account, tying costs to the responsible teams is challenging.\n",
        "\n",
        "Implement a standard identification method for cloud resource ownership – for example, apply resource tags, not just naming standards. Tags and labels make identifying resources and their owners throughout their lifecycle easier. Align cost centers to the required reporting granularity (i.e., department or individual user).\n",
        "\n",
        "Large organizations often use configuration management databases to track tagged cloud resources. Metadata also helps you optimize costs and prioritize resources (i.e., lower underutilization thresholds for less critical resources). Resource tags should include contacts for the resource owner to facilitate communication."
      ],
      "metadata": {
        "id": "YlrGpLgSsv-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. Q: How do you ensure cost optimization while maintaining high-performance levels in a machine learning project?"
      ],
      "metadata": {
        "id": "8wFldXJlt-1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here are some key benefits of incorporating machine learning in predictive maintenance:\n",
        "\n",
        "Increased accuracy. Machine learning algorithms can build complex models by analyzing and learning from vast amounts of data. These models can predict failures with higher accuracy, ensuring maintenance is performed only when necessary and reducing the risk of unexpected equipment breakdowns.\n",
        "Proactive decision-making. Unlike traditional methods, machine learning-based predictive maintenance allows companies to take proactive actions by identifying subtle patterns and trends in equipment behavior. This level of anticipation helps to minimize downtime and optimize maintenance planning.\n",
        "Adaptability and scalability. Machine learning algorithms can quickly adapt to new data trends and evolving system parameters, providing a more flexible and scalable solution than static, rule-based approaches.\n",
        "Cost efficiency. Organizations can allocate resources more efficiently by accurately predicting the likelihood of equipment failures and targeting only high-risk assets, saving time and maintenance costs."
      ],
      "metadata": {
        "id": "DGK2r5Kxt-6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}