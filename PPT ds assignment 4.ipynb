{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "espDNw-tLxrN"
      },
      "outputs": [],
      "source": [
        "1. What is the purpose of the General Linear Model (GLM)?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GLM models allow us to build a linear relationship between the response and predictors, even though their underlying relationship is not linear\n",
        "\n",
        "Some of the features of GLMs include:\n",
        "Flexibility: GLMs can model a wide range of relationships between the response and predictor variables, including linear, logistic, Poisson, and exponential relationships.\n",
        "\n",
        "Model interpretability: GLMs provide a clear interpretation of the relationship between the response and predictor variables, as well as the effect of each predictor on the response.\n",
        "\n",
        "Robustness: GLMs can be robust to outliers and other anomalies in the data, as they allow for non-normal distributions of the response variable.\n",
        "\n",
        "Scalability: GLMs can be used for large datasets and complex models, as they have efficient algorithms for model fitting and prediction.\n",
        "\n",
        "Ease of use: GLMs are relatively easy to understand and use, especially compared to more complex models such as neural networks or decision trees.\n",
        "\n",
        "Hypothesis testing: GLMs allow for hypothesis testing and statistical inference, which can be useful in many applications where it’s important to understand the\n",
        " significance of relationships between variables.\n",
        "\n",
        "Regularization: GLMs can be regularized to reduce overfitting and improve model performance, using techniques such as Lasso, Ridge, or Elastic Net regression.\n",
        "\n",
        "Model comparison: GLMs can be compared using information criteria such as AIC or BIC, which can help to choose the best model among a set of alternatives."
      ],
      "metadata": {
        "id": "fuAlLJKeaE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. What are the key assumptions of the General Linear Model?"
      ],
      "metadata": {
        "id": "JybzebCzbB8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Similar to Linear Regression Model, there are some basic assumptions for Generalized Linear Models as well. Most of the assumptions are similar to Linear Regression models, while some of the assumptions of Linear Regression are modified.\n",
        "\n",
        "Data should be independent and random (Each Random variable has the same probability distribution).\n",
        "The response variable y does not need to be normally distributed, but the distribution is from an exponential family (e.g. binomial, Poisson, multinomial, normal)\n",
        "The original response variable need not have a linear relationship with the independent variables, but the transformed response variable (through the link function) is linearly dependent on the independent variables\n",
        "Ex., Logistic Regression Equation,  Log odds = β0+β1X1+β2X2 ,\n",
        "\n",
        "where β0,β1,β2 are regression coefficient, and X1,X2 are the independent variables"
      ],
      "metadata": {
        "id": "94fqJecba4Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. How do you interpret the coefficients in a GLM?"
      ],
      "metadata": {
        "id": "S_zT0aMccJWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In linear models, the interpretation of model parameters is linear. For example, if a you were modelling plant height against altitude and your coefficient\n",
        "for altitude was -0.9, then plant height will decrease by 1.09 for every increase in altitude of 1 unit.\n",
        "\n",
        "For generalised linear models, the interpretation is not this straightforward. Here, I will explain how to interpret the co-efficients in generalised linear\n",
        " models (glms). First you will want to read our pages on glms for binary and count data page on interpreting coefficients in linear models."
      ],
      "metadata": {
        "id": "uC1ej6S3cJcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What is the difference between a univariate and multivariate GLM?"
      ],
      "metadata": {
        "id": "zINOA1JihI72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The most basic difference is that univariate regression has one explanatory (predictor) variable x and multivariate regression has more at least\n",
        " two explanatory (predictor) variables x1,x2,...,xn . In both situations there is one response variable y ."
      ],
      "metadata": {
        "id": "HniNHMoKhW0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Explain the concept of interaction effects in a GLM."
      ],
      "metadata": {
        "id": "DB3mJmLFhW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ". In general, the existence of an interaction means that the effect of one variable depends on the value of the other variable with which it interacts.\n",
        " If there isn't an interaction, then the value of the other variable doesn't matter."
      ],
      "metadata": {
        "id": "AwmwBtbThW6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. How do you handle categorical predictors in a GLM?"
      ],
      "metadata": {
        "id": "wnDyOwZUhW9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The first way to exploit a categorical predictor is to add it to the regression model. If there are more than two categories, you need to break down the variable\n",
        " into multiple binary ones.\n",
        "\n",
        "The other approach called ANOVA stands for Analysis of Variance. This method was formerly introduced in Fisher's 1925 classic book Statistical Methods for Research Workers.\n",
        " ANOVA is used to analyze the differences in the mean of groups of samples with respect to categories. It is a way to extend the t-test\n",
        "to multiple groups and categories."
      ],
      "metadata": {
        "id": "SQ9B7v-ZhXAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. What is the purpose of the design matrix in a GLM?"
      ],
      "metadata": {
        "id": "8XEfDwDyjlBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The purpose of the design matrix is to allow models that further constrain parameter sets. These constraints provide additional flexibility in modeling\n",
        "and allows researchers to build models that cannot be derived using the simple PIMs in ."
      ],
      "metadata": {
        "id": "thE3uYuFhXDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. How do you test the significance of predictors in a GLM?"
      ],
      "metadata": {
        "id": "VOoIzp7ShXGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "We can decide whether there is any significant relationship between the dependent variable y and the independent variables xk (k = 1, 2, ..., p)\n",
        "in the logistic regression equation. In particular, if any of the null hypothesis that βk = 0 (k = 1, 2, ..., p) is valid, then xk is statistically insignificant\n",
        " in the logistic regression model."
      ],
      "metadata": {
        "id": "e7ztVN89hXND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"
      ],
      "metadata": {
        "id": "dfxQTdJ3lGJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Type I sum of squares are “sequential.” In essence the factors are tested in the order they are listed in the model.\n",
        "Type III sum of squares are “partial.” In essence, every term in the model is tested in light of every other term in the model.\n",
        "\n",
        "It turns out that there is not just one way to calculate ANOVAs. In fact, there are three different types - called, Type 1, 2, and 3 (or Type I, II and III).\n",
        " These types differ in how they calculate variability (specifically the sums of of squares). If your data is relatively balanced, meaning tha\n",
        " t there are relatively equal numbers of observations in each group, then all three types will give you the same answer. However, if your data are unbalanced,\n",
        " meaning that some groups of data have many more observations than others, then you need to use Type II (2) or Type III (3).\n",
        "\n",
        "The standard aov() function in base-R uses Type I sums of squares. Therefore, it is only appropriate when your data are balanced. If your\n",
        " data are unbalanced, you should conduct an ANOVA with Type II or Type III sums of squares. To do this, you can use the Anova() function\n",
        " in the car package. The Anova() function has an argument called type that allows you to specify the type of ANOVA you want to calculate."
      ],
      "metadata": {
        "id": "Yyg6QVyxlGMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. Explain the concept of deviance in a GLM."
      ],
      "metadata": {
        "id": "v95uufVOmX6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Deviance is a measure of error; lower deviance means better fit to data.\n",
        "The greater the deviance, the worse the model fits compared to the best case (saturated).\n",
        "Deviance is a quality-of-fit statistic for a model that is often used for statistical hypothesis testing."
      ],
      "metadata": {
        "id": "8tWA_TL8lGRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. What is regression analysis and what is its purpose?"
      ],
      "metadata": {
        "id": "-bWiOGwFlGUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship\n",
        "between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).\n",
        "\n",
        "Also called simple regression or ordinary least squares (OLS), linear regression is the most common form of this technique. Linear regression establishes the\n",
        " linear relationship between two variables based on a line of best fit. Linear regression is thus graphically depicted using a straight line with the slope\n",
        "  defining how the change in one variable impacts a change in the other. The y-intercept of a linear regression relationship represents the value of one\n",
        "   variable when the value of the other is zero. Non-linear regression models also exist, but are far more complex.\n",
        "\n",
        "Regression analysis is a powerful tool for uncovering the associations between variables observed in data, but cannot easily indicate causation.\n",
        "It is used in several contexts in business, finance, and economics. For instance, it is used to help investment managers value assets and understand\n",
        "the relationships between factors such as commodity prices and the stocks of businesses dealing in those commodities.\n",
        "\n",
        "\n",
        "\n",
        "In statistical analysis, regression is used to identify the associations between variables occurring in some data. It can show both the magnitude of such an\n",
        "association and also determine its statistical significance (i.e., whether or not the association is likely due to chance). Regression is a powerful tool for\n",
        " statistical inference and has also been used to try to predict future outcomes based on past observations."
      ],
      "metadata": {
        "id": "hntUVVfAlGYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. What is the difference between simple linear regression and multiple linear regression?"
      ],
      "metadata": {
        "id": "uQo0JUUuoeVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Simple linear regression has only one x and one y variable.\n",
        "\n",
        "Multiple linear regression has one y and two or more x variables.\n",
        "\n",
        "For instance, when we predict rent based on square feet alone that is simple linear regression.\n",
        "\n",
        "When we predict rent based on square feet and age of the building that is an example of multiple linear regression."
      ],
      "metadata": {
        "id": "xe1aG6eBoi6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. How do you interpret the R-squared value in regression?"
      ],
      "metadata": {
        "id": "l1acLTogouzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Regression Analysis is a set of statistical processes that are at the core of data science. In the field of numerical simulation, it represents\n",
        " the most well-understood models and helps in interpreting machine learning algorithms. Their real-life applications can be seen in a wide range of domains,\n",
        " ranging from advertising and medical research to agricultural science and even different sports. In linear regression models, r squared interpretation is\n",
        "  a goodness-fit-measure. It takes into account the strength of the relationship between the model and the dependent variable. Its convenience is measured on a scale of 0 – 100%."
      ],
      "metadata": {
        "id": "KQ38Mm_Wo1rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. What is the difference between correlation and regression?"
      ],
      "metadata": {
        "id": "gaFa2FvXpFCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Correlation and Regression are the two analysis based on multivariate distribution. A multivariate distribution is described as a distribution of multiple variables.\n",
        " Correlation is described as the analysis which lets us know the association or the absence of the relationship between two variables ‘x’ and ‘y’. On the other end,\n",
        " Regression analysis, predicts the value of the dependent variable based on the known value of the independent variable, assuming that average mathematical\n",
        " relationship between two or more variables.\n",
        "\n",
        " The points given below, explains the difference between correlation and regression in detail:\n",
        "\n",
        "A statistical measure which determines the co-relationship or association of two quantities is known as Correlation. Regression describes how an independent variable\n",
        " is numerically related to the dependent variable.\n",
        "\n",
        "Correlation is used to represent the linear relationship between two variables. On the contrary, regression is used to fit the best line and estimate\n",
        " one variable on the basis of another variable.\n",
        "\n",
        "In correlation, there is no difference between dependent and independent variables i.e. correlation between x and y is similar to y and x.\n",
        " Conversely, the regression of y on x is different from x on y.\n",
        "\n",
        "Correlation indicates the strength of association between variables. As opposed to, regression reflects the impact of the unit\n",
        "change in the independent variable on the dependent variable.\n",
        "\n",
        "Correlation aims at finding a numerical value that expresses the relationship between variables. Unlike regression whose goal is to\n",
        " predict values of the random variable on the basis of the values of fixed variable."
      ],
      "metadata": {
        "id": "xgh5CFoKoi90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. What is the difference between the coefficients and the intercept in regression?"
      ],
      "metadata": {
        "id": "PdFkze9Fp11G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P values and coefficients in regression analysis work together to tell you which relationships in your model are statistically significant and the nature of\n",
        " those relationships. The linear regression coefficients describe the mathematical relationship between each independent variable and the dependent variable.\n",
        "  The p values for the coefficients indicate whether these relationships are statistically significant."
      ],
      "metadata": {
        "id": "ieacZ0DkpZIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. How do you handle outliers in regression analysis?"
      ],
      "metadata": {
        "id": "kc0JiXZ2pZLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "There are many possible approaches to dealing with outliers: removing them from the observations, treating them (for example, capping the extreme observations\n",
        "at a reasonable value), or using algorithms that are well-suited for dealing with such values on their own.\n",
        "\n",
        "\n",
        "Linear regression is one of the simplest machine learning models out there. It is often the starting point not only for learning about data science but also\n",
        " for building quick and simple minimum viable products (MVPs), which then serve as benchmarks for more complex algorithms.\n",
        "\n",
        "In general, linear regression fits a line (in two dimensions) or a hyperplane (in three and more dimensions) that best describes the linear relationship\n",
        "between the features and the target value. The algorithm also assumes that the probability distributions of the features are well-behaved; for example,\n",
        "they follow the Gaussian distribution.\n",
        "\n",
        "Outliers are values that are located far outside of the expected distribution. They cause the distributions of the features to be less well-behaved.\n",
        "As a consequence, the model can be skewed towards the outlier values, which, as I’ve already established, are far away from the central mass of observations.\n",
        " Naturally, this leads to the linear regression finding a worse and more biased fit with inferior predictive performance.\n",
        "\n",
        "It is important to remember that the outliers can be found both in the features and the target variable, and all the scenarios can worsen the performance of the model."
      ],
      "metadata": {
        "id": "K7cmoeQBpZOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. What is the difference between ridge regression and ordinary least squares regression?"
      ],
      "metadata": {
        "id": "PChSImK7pZRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ordinary Least Squares (‘OLS’) is one of the oldest and most simple algorithms used for regression.They are particularly useful when there is not a huge\n",
        " amount of observations, or when the inputs reliably predict the response (low signal to noise ratio).\n",
        "\n",
        " Ridge regression works with an enhanced cost function when compared to the least squares cost function. Instead of the simple sum of squares,\n",
        " Ridge regression introduces an additional ‘regularization’ parameter that penalizes the size of the weights."
      ],
      "metadata": {
        "id": "VDvU2cO_pZUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. What is heteroscedasticity in regression and how does it affect the model?"
      ],
      "metadata": {
        "id": "TfV3AgjUpZXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "we can define heteroscedasticity as the condition in which the variance of error term or the residual term in a regression model varies. As you can see in\n",
        "the above diagram, in case of homoscedasticity, the data points are equally scattered while in case of heteroscedasticity the data points are not equally scattered.\n",
        "\n",
        "Effects of Heteroscedasticity:\n",
        "\n",
        "As mentioned above that one of the assumption (assumption number 2) of linear regression is that there is no heteroscedasticity. Breaking this\n",
        "assumption means that OLS (Ordinary Least Square) estimators are not the Best Linear Unbiased Estimator(BLUE) and their variance is not the lowest\n",
        " of all other unbiased estimators.\n",
        "\n",
        "Estimators are no longer best/efficient.\n",
        "\n",
        "The tests of hypothesis (like t-test, F-test) are no longer valid due to the inconsistency in the co-variance matrix of the estimated regression coefficients."
      ],
      "metadata": {
        "id": "rH8PDPN3pZar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. How do you handle multicollinearity in regression analysis?"
      ],
      "metadata": {
        "id": "RlI-jhpPvIX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "There are many reasons why multicollinearity may occur.\n",
        "\n",
        "It may occur as a result of:\n",
        "\n",
        "The inclusion of identical variables. For example, one may have identical variables in a dataset, such as mass in kilograms and mass in pounds.\n",
        "\n",
        "Creation of new variables that are dependent on others. When we create variables that depend on other variables, we introduce redundant information to the model.\n",
        " As such, we may inadvertently encourage the occurrence of multicollinearity.\n",
        "\n",
        "Inadequate data. In some cases, when the data is inadequate, we may experience multicollinearity. This is due to the small sample size, which might,\n",
        "in turn, experience great variance.\n",
        "\n",
        "Addressing multicollinearity\n",
        "\n",
        "If we conclude that multicollinearity poses a problem for our regression model, we can attempt a handful of basic fixes.\n",
        "\n",
        "Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation.\n",
        "This assists in reducing the multicollinearity linking correlated features. It is advisable to get rid of variables iteratively. We would begin\n",
        " with a variable with the highest VIF score since other variables are likely to capture its trend. As a result of removing this variable,\n",
        "  other variables’ VIF values are likely to reduce.\n",
        "\n",
        "More data. Statistically, a regression model with more data is likely to suffer less variance due to a larger sample size. This will reduce the impact of multicollinearity."
      ],
      "metadata": {
        "id": "u8x78E_ovIan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. What is polynomial regression and when is it used?"
      ],
      "metadata": {
        "id": "0OsqWGinvIdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear\n",
        "regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the\n",
        " complexity of the relationship."
      ],
      "metadata": {
        "id": "nfBtlt14vIf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21. What is a loss function and what is its purpose in machine learning?"
      ],
      "metadata": {
        "id": "C9CY7CdQvIiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loss functions are used to determine the error (aka “the loss”) between the output of our algorithms and the given target value.  In layman’s terms,\n",
        "the loss function expresses how far off the mark our computed output is.\n",
        "\n",
        "loss functions help gauge how a machine learning model is performing with its given data, and how well it’s able to predict an expected outcome.\n",
        " Many machine learning algorithms use loss functions in the optimization process during training to evaluate and improve its output accuracy.\n",
        "  Also, by minimizing a chosen loss function during optimization, this can help determine the best model parameters needed for given data."
      ],
      "metadata": {
        "id": "jy35et8tvIkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22. What is the difference between a convex and non-convex loss function?"
      ],
      "metadata": {
        "id": "t8Zuc3S_vImw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A convex function is one in which a line drawn between any two points on the graph lies on the graph or above it. There is only one requirement.\n",
        "\n",
        "A non-convex function is one in which a line drawn between any two points on the graph may cross additional points. It was described as “wavy.”\n",
        "\n",
        "When a cost function is non-convex, it has a higher chance of finding local minima rather than the global minimum, which is usually undesirable in\n",
        " machine learning models from an optimization standpoint."
      ],
      "metadata": {
        "id": "P-F4U8SQvIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. What is mean squared error (MSE) and how is it calculated?"
      ],
      "metadata": {
        "id": "Y4RIYNnPupEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mean squared error (MSE) measures the amount of error in statistical models. It assesses the average squared difference between the observed and predicted\n",
        " values. When a model has no error, the MSE equals zero. As model error increases, its value increases. The mean squared error\n",
        "  is also known as the mean squared deviation (MSD).\n",
        "\n",
        "For example, in regression, the mean squared error represents the average squared residual\n",
        "\n",
        "The calculations for the mean squared error are similar to the variance. To find the MSE, take the observed value, subtract the predicted value, and square that\n",
        " difference. Repeat that for all observations. Then, sum all of those squared values and divide by the number of observations.\n",
        "\n",
        "Notice that the numerator is the sum of the squared errors (SSE), which linear regression\n"
      ],
      "metadata": {
        "id": "7xgqQmaiupNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24. What is mean absolute error (MAE) and how is it calculated?"
      ],
      "metadata": {
        "id": "Yv4i3V7HymSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The MAE score is measured as the average of the absolute error values. The Absolute is a mathematical function that makes a number positive. Therefore,\n",
        " the difference between an expected value and a predicted value can be positive or negative and will necessarily be positive when calculating the MAE.\n",
        "\n",
        "The MAE score is measured as the average of the absolute error values. The Absolute is a mathematical function that makes a number positive.\n",
        " Therefore, the difference between an expected value and a predicted value can be positive or negative and will necessarily be positive when calculating the MAE."
      ],
      "metadata": {
        "id": "Aybfr1KJymU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25. What is log loss (cross-entropy loss) and how is it calculated?"
      ],
      "metadata": {
        "id": "e7JTn-umymXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual\n",
        " observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "BgENGL0Kymad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26. How do you choose the appropriate loss function for a given problem?"
      ],
      "metadata": {
        "id": "Rc5kYEP7ymdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The Loss function is directly related to the predictions of your model that you have built. So if your loss function value is less, your model will\n",
        " be providing good results. Loss function or I should rather say, the Cost function that is used to evaluate the model performance,\n",
        "  needs to be minimized in order to improve its performance.\n",
        "\n",
        "Lets now dive into the Loss functions.\n",
        "\n",
        "Widely speaking, the Loss functions can be grouped into two major categories concerning the types of problems that we come across in the\n",
        " real world — Classification and Regression. In Classification, the task is to predict the respective probabilities of all classes that the problem\n",
        " is dealing with. In Regression, oppositely, the task is to predict the continuous value concerning a given set of independent features to the learning algorithm.\n",
        "\n",
        "Assumptions:\n",
        "    n/m — Number of training samples.\n",
        "    i — ith training sample in a dataset.\n",
        "    y(i) — Actual value for the ith training sample.\n",
        "    y_hat(i) — Predicted value for the ith training sample.\n"
      ],
      "metadata": {
        "id": "RJsv7pRfymga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27. Explain the concept of regularization in the context of loss functions."
      ],
      "metadata": {
        "id": "oYAZunloymns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " In machine learning, regularization refers to minimizing or reducing the coefficient estimates towards zero to prevent the machine learning model from underfitting.\n",
        "\n",
        "The ‘coefficients’ for the input parameters are also included in those voluminous books on machine learning models. The machine learning model generally\n",
        " assigns larger importance to a parameter if its coefficient is higher. Therefore, regularization in machine learning entails modifying these coefficients\n",
        "  by altering their magnitude and decreasing them to impose generalization.\n",
        "\n",
        "Machine learning involves equipping computers to perform specific tasks without explicit instructions. So, the systems are programmed to learn\n",
        " and improve from experience automatically"
      ],
      "metadata": {
        "id": "mNOH5_ANupQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "28. What is Huber loss and how does it handle outliers?"
      ],
      "metadata": {
        "id": "jEsfJ151pZeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification\n",
        "is also sometimes used."
      ],
      "metadata": {
        "id": "LMZzqBdAhJCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "29. What is quantile loss and when is it used?"
      ],
      "metadata": {
        "id": "tZdpfQeJ3nb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Machine learning algorithms aiming to predict a particular variable quantile use quantile loss as the loss function. Before going to the formulation,\n",
        " let us consider a simple example.\n",
        "\n",
        "Imagine a problem where the goal is to predict the 75-th percentile of a variable. In fact, this statement is equivalent to the one that prediction errors\n",
        " have to be negative in 75% of cases and in the other 25% to be positive. That is the actual intuition used behind the quantile loss.\n",
        "\n",
        " We have discovered quantile loss — a flexible loss function that can be incorporated into any regression model to predict a certain variable quantile.\n",
        "  Based on the example of LightGBM, we saw how to adjust a model, so it solves a quantile regression problem."
      ],
      "metadata": {
        "id": "YNbKFqeo3nfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "30. What is the difference between squared loss and absolute loss?"
      ],
      "metadata": {
        "id": "9JyDmGtv6I4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mean Square Error Loss is also known a L2 regularization and is used for Regression tasks. It tells you how close a regression line is to a set of data points.\n",
        "It calculates the square difference between the current output and the expected output divided by the number of output. However,\n",
        " Mean Square Error loss is more sensitive to outliers due to using the square difference.\n",
        "\n",
        " Mean Absolute Error is also known as L1 regularization and is used for Regression tasks. It computes the mean of squares of errors between labeled data and predicted data.\n",
        "\n",
        "It calculates the absolute difference between the current output and the expected output divided by the number of output. It’s aim is to minimise\n",
        " this absolute differences. Mean Absolute Error is not sensitive towards outliers as it is based on absolute value, unlike Mean Square Error."
      ],
      "metadata": {
        "id": "qOle5bKr6I7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "31. What is an optimizer and what is its purpose in machine learning?"
      ],
      "metadata": {
        "id": "zh2AGGcy7Qrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The optimizer is a crucial element in the learning process of the ML model. PyTorch itself has 13 optimizers, making it challenging and overwhelming\n",
        " to pick the right one for the problem"
      ],
      "metadata": {
        "id": "Z8sYcjGn6I95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Optimization plays an important part in a machine learning project in addition to fitting the learning algorithm on the training dataset.\n",
        "\n",
        "The step of preparing the data prior to fitting the model and the step of tuning a chosen model also can be framed as an optimization problem. In fact,\n",
        " an entire predictive modeling project can be thought of as one large optimization problem."
      ],
      "metadata": {
        "id": "qp-2317M6JBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data Preparation as Optimization\n",
        "\n",
        "Hyperparameter Tuning as Optimization\n",
        "\n",
        "Model Selection as Optimization\n"
      ],
      "metadata": {
        "id": "XPcn-1wp5_xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCym7EWm8jrc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}