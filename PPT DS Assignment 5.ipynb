{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zGYejXHxIGe"
      },
      "outputs": [],
      "source": [
        "1. What is the Naive Approach in machine learning?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction"
      ],
      "metadata": {
        "id": "BeFxKNpfx5y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Explain the assumptions of feature independence in the Naive Approach."
      ],
      "metadata": {
        "id": "t0KxVneXx7PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The assumption of feature independence in Naive Bayes simplifies the computation and makes the algorithm more efficient. By assuming independence, the joint probability of features given the class can be computed as the product of individual feature probabilities."
      ],
      "metadata": {
        "id": "oGhUP3EEyFez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. How does the Naive Approach handle missing values in the data?"
      ],
      "metadata": {
        "id": "9eqkxzmqyLp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Deleting Rows with missing values\n",
        "Impute missing values for continuous variable\n",
        "Impute missing values for categorical variable\n",
        "Other Imputation Methods\n",
        "Using Algorithms that support missing values\n",
        "Prediction of missing values\n",
        "Imputation using Deep Learning Library — Datawig"
      ],
      "metadata": {
        "id": "gH_gIERLyfBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. What are the advantages and disadvantages of the Naive Approach?"
      ],
      "metadata": {
        "id": "zqqgqVNayi4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Advantages and Disadvantages of Naive Bayes\n",
        "Advantages\n",
        "This algorithm works quickly and can save a lot of time.\n",
        "Naive Bayes is suitable for solving multi-class prediction problems.\n",
        "If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data.\n",
        "Naive Bayes is better suited for categorical input variables than numerical variables.\n",
        "Disadvantages\n",
        "Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life. This limits the applicability of this algorithm in real-world use cases.\n",
        "This algorithm faces the ‘zero-frequency problem’ where it assigns zero probability to a categorical variable whose category in the test data set wasn’t available in the training dataset. It would be best if you used a smoothing technique to overcome this issue.\n",
        "Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously."
      ],
      "metadata": {
        "id": "aYBOtZcn7KBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Can the Naive Approach be used for regression problems? If yes, how?"
      ],
      "metadata": {
        "id": "AHQ6fk4a7SdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems, though with some modifications, it can also be used for solving regression problems."
      ],
      "metadata": {
        "id": "0q2gH4du7Tge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. How do you handle categorical features in the Naive Approach?"
      ],
      "metadata": {
        "id": "LWkkKcJh8Fa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "One Hot Encoding\n",
        "\n",
        "· One Hot Encoding with multiple categories\n",
        "\n",
        "· Ordinal Number Encoding\n",
        "\n",
        "· Count or Frequency Encoding\n",
        "\n",
        "· Target guided Ordinal Encoding\n",
        "\n",
        "· Mean Ordinal Encoding\n",
        "\n",
        "· Probability Ratio Encoding"
      ],
      "metadata": {
        "id": "0lU2ya_W8QDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. What is Laplace smoothing and why is it used in the Naive Approach?"
      ],
      "metadata": {
        "id": "rf5_1lfl8QGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews."
      ],
      "metadata": {
        "id": "lgIKjo8y8lhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. How do you choose the appropriate probability threshold in the Naive Approach?\n"
      ],
      "metadata": {
        "id": "PSUkUXFX8tNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "You should look for every positively classified documents in each and every class from the learning set. You have to look for the minimum probability required to classify particular document truly in its class. You can use this minimum probability as threshold for the classification model."
      ],
      "metadata": {
        "id": "HeBb7s2m8tXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Give an example scenario where the Naive Approach can be applied."
      ],
      "metadata": {
        "id": "UVnNz3GR9R2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " It is the simplest method which uses brute force approach.\n",
        "\n",
        "ii) It is a straight forward approach of solving the problem.\n",
        "\n",
        "iii) It compares first character of pattern with searchable text. If match is found, pointers in both strings are advanced. If match not found, pointer of text is incremented and pointer ofpattern is reset. This process is repeated until the end of the text.\n",
        "\n",
        "iv) It does not require any pre-processing. It directly starts comparing both strings character by character."
      ],
      "metadata": {
        "id": "-1faJCjS9R6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. What is the K-Nearest Neighbors (KNN) algorithm?"
      ],
      "metadata": {
        "id": "f0hbsLca9YDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
      ],
      "metadata": {
        "id": "vGW0WRvn9emB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. How does the KNN algorithm work?"
      ],
      "metadata": {
        "id": "NDJypv459oGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First, the distance between the new point and each training point is calculated.\n",
        "The closest k data points are selected (based on the distance). ...\n",
        "The average of these data points is the final prediction for the new point."
      ],
      "metadata": {
        "id": "k-V6kbRR9oNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. How do you choose the value of K in KNN?"
      ],
      "metadata": {
        "id": "dqn3H9NK940b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better."
      ],
      "metadata": {
        "id": "IxyXrl8z947g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. What are the advantages and disadvantages of the KNN algorithm?"
      ],
      "metadata": {
        "id": "pSRVQAQQ8QJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Advantages\t                           Disadvantages\n",
        "\n",
        "Simple and Easy to Understand\t         Sensitive to Outliers\n",
        "Non-parametric                        \tComputationally Expensive\n",
        "No Training Required                   \tRequires Good Choice of K\n",
        "Can Handle Large Datasets              \tLimited to Euclidean Distance\n",
        "Accurate and Effective\t                Imbalanced Data"
      ],
      "metadata": {
        "id": "Mp_fxB4r-c5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. How does the choice of distance metric affect the performance of KNN?"
      ],
      "metadata": {
        "id": "q-_6pkPw-c8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree."
      ],
      "metadata": {
        "id": "mGgJ-aDG-c_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15. Can KNN handle imbalanced datasets? If yes, how?"
      ],
      "metadata": {
        "id": "Ts5WC96E-dB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In principal, unbalanced classes are not a problem at all for the k-nearest neighbor algorithm.\n",
        "\n",
        "Because the algorithm is not influenced in any way by the size of the class, it will not favor any on the basis of size. Try to run k-means with an obvious outlier and k+1 and you will see that most of the time the outlier will get its own class.\n",
        "\n",
        "Of course, with hard datasets it is always advisable to run the algorithm multiple times. This is to avoid trouble due to a bad initialization."
      ],
      "metadata": {
        "id": "OOzku3-s-dFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. How do you handle categorical features in KNN?"
      ],
      "metadata": {
        "id": "lfqBtyxR-dID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "You have to decide how to convert categorical features to a numeric scale, and somehow assign inter-category distances in a way that makes sense with other features (like, age-age distances...but what is an age-category distance?)."
      ],
      "metadata": {
        "id": "mY2xJxUO-dK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. What are some techniques for improving the efficiency of KNN?"
      ],
      "metadata": {
        "id": "0pxrI_Sr-dOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model."
      ],
      "metadata": {
        "id": "rB-XWrsF8QMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. Give an example scenario where KNN can be applied."
      ],
      "metadata": {
        "id": "m70TNlRz8QOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party 'Congress', “Will Vote to Party 'BJP'. Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition."
      ],
      "metadata": {
        "id": "ZIp6GeUJ8QSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. What is clustering in machine learning?"
      ],
      "metadata": {
        "id": "gbthsiHV_yrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups."
      ],
      "metadata": {
        "id": "umynI5zH_yuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. Explain the difference between hierarchical clustering and k-means clustering."
      ],
      "metadata": {
        "id": "0q5B756G_yxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of ‘K’.\n",
        "\n",
        "Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster.\n",
        "\n",
        "Main differences between K means and Hierarchical Clustering are:\n",
        "\n",
        "k-means Clustering\t                                                               Hierarchical Clustering\n",
        "\n",
        "k-means, using a pre-specified  number                                             Hierarchical methods can be either divisive or agglomerative.\n",
        " of clusters, the method  assigns\n",
        "  records to each cluster to  find\n",
        "  the mutually exclusive cluster  of\n",
        "   spherical shape based on distance.\n",
        "\n",
        "   K Means clustering needed advance                                                      In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.\n",
        "    knowledge of K i.e. no. of clusters\n",
        " one want to divide your data.\n",
        "\n"
      ],
      "metadata": {
        "id": "kcLA5aXk_yzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21. How do you determine the optimal number of clusters in k-means clustering?"
      ],
      "metadata": {
        "id": "WvPrG8YWA0mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n",
        "For each k, calculate the total within-cluster sum of square (wss).\n",
        "Plot the curve of wss according to the number of clusters k.\n",
        "The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters."
      ],
      "metadata": {
        "id": "w6c384tD_y2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22. What are some common distance metrics used in clustering?"
      ],
      "metadata": {
        "id": "gUHMV_Gw_y4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance."
      ],
      "metadata": {
        "id": "LfIu14w3_y7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. How do you handle categorical features in clustering?"
      ],
      "metadata": {
        "id": "b4rCGgZb_zAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "One Hot Encoding\n",
        "\n",
        "· One Hot Encoding with multiple categories\n",
        "\n",
        "· Ordinal Number Encoding\n",
        "\n",
        "· Count or Frequency Encoding\n",
        "\n",
        "· Target guided Ordinal Encoding\n",
        "\n",
        "· Mean Ordinal Encoding\n",
        "\n",
        "· Probability Ratio Encoding"
      ],
      "metadata": {
        "id": "CWCFXGyj_zDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24. What are the advantages and disadvantages of hierarchical clustering?"
      ],
      "metadata": {
        "id": "-WA_kpTu_zGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets. And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets"
      ],
      "metadata": {
        "id": "YwMGOF8fB0LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "25. Explain the concept of silhouette score and its interpretation in clustering."
      ],
      "metadata": {
        "id": "E6o9YiPJB0N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n",
        "\n",
        "1: Means clusters are well apart from each other and clearly distinguished.\n",
        "\n",
        "0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "\n",
        "-1: Means clusters are assigned in the wrong way."
      ],
      "metadata": {
        "id": "jYa2r1xaB0RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "26. Give an example scenario where clustering can be applied."
      ],
      "metadata": {
        "id": "xQngLAv6B0UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "For instance, clustering can be used to identify several types of depression. Cluster analysis is also used to identify patterns in the spatial or temporal allocation of a disease. Business − Businesses collect huge amounts of data on current and potential users."
      ],
      "metadata": {
        "id": "63rHIEdhB0W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27. What is anomaly detection in machine learning?"
      ],
      "metadata": {
        "id": "9opjZIaYB0ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anomaly detection (aka outlier analysis) is a step in data mining that identifies data points, events, and/or observations that deviate from a dataset's normal behavior."
      ],
      "metadata": {
        "id": "clnzCThuC1Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "28. Explain the difference between supervised and unsupervised anomaly detection."
      ],
      "metadata": {
        "id": "aE4JVhaYC1JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The main difference between supervised and unsupervised anomaly detection is the approach involved, where supervised approach makes use of predefined algorithms and AI training, while unsupervised approach uses a general outlier-detection mechanism based on pattern matching."
      ],
      "metadata": {
        "id": "jz-60h2qC1MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "29. What are some common techniques used for anomaly detection?"
      ],
      "metadata": {
        "id": "eojZ7PcRC1Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Statistical (Z-score, Tukey's range test and Grubbs's test)\n",
        "Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)\n",
        "Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."
      ],
      "metadata": {
        "id": "XoEk_-ueC1Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "30. How does the One-Class SVM algorithm work for anomaly detection?"
      ],
      "metadata": {
        "id": "Fyvrd-v8DZ-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method."
      ],
      "metadata": {
        "id": "63hdkBtxDbBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "31. How do you choose the appropriate threshold for anomaly detection?"
      ],
      "metadata": {
        "id": "UvVVh4E3DbDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "You can make find the ideal thresholds by plotting an RC curve with TP and FP rates. The way to tune the anomaly detection threshold is as follows: Construct a train set using a large sample of observations without anomalies"
      ],
      "metadata": {
        "id": "1GHprmmEDbGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "32. How do you handle imbalanced datasets in anomaly detection?"
      ],
      "metadata": {
        "id": "m6kLmaWgDbJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Approach to deal with the imbalanced dataset problem\n",
        "Choose Proper Evaluation Metric.\n",
        "Resampling (Oversampling and Undersampling)\n",
        "SMOTE.\n",
        "BalancedBaggingClassifier."
      ],
      "metadata": {
        "id": "YnnYDic-DbhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "33. Give an example scenario where anomaly detection can be applied."
      ],
      "metadata": {
        "id": "2uDkNnhlDbkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anomaly Detection Examples\n",
        "\n",
        "For example, a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer."
      ],
      "metadata": {
        "id": "wNlYxD35DbmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "34. What is dimension reduction in machine learning?"
      ],
      "metadata": {
        "id": "_ZirImHODbph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible. This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data."
      ],
      "metadata": {
        "id": "CjkfceUWDbtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "35. Explain the difference between feature selection and feature extraction."
      ],
      "metadata": {
        "id": "-HV1eNb1DbzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space."
      ],
      "metadata": {
        "id": "i9mLcbtsDaBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "36. How does Principal Component Analysis (PCA) work for dimension reduction?"
      ],
      "metadata": {
        "id": "UwbxAW17DaD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one."
      ],
      "metadata": {
        "id": "cCKOFBKWDaGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "37. How do you choose the number of components in PCA?"
      ],
      "metadata": {
        "id": "IlL3dkw-DaJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
      ],
      "metadata": {
        "id": "DccvaTFPDaLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "38. What are some other dimension reduction techniques besides PCA?"
      ],
      "metadata": {
        "id": "j1rRBEBpDaPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "There are several techniques for dimensionality reduction, including principal component analysis (PCA), singular value decomposition (SVD), and linear discriminant analysis (LDA). Each technique uses a different method to project the data onto a lower-dimensional space while preserving important information."
      ],
      "metadata": {
        "id": "GXch1C1yC1VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "39. Give an example scenario where dimension reduction can be applied."
      ],
      "metadata": {
        "id": "eW1NZ-64FMAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics."
      ],
      "metadata": {
        "id": "Wdy-bfKCFMDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "40. What is feature selection in machine learning?"
      ],
      "metadata": {
        "id": "0OL_koACFMF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve."
      ],
      "metadata": {
        "id": "yjPqnZvuFMIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "41. Explain the difference between filter, wrapper, and embedded methods of feature selection."
      ],
      "metadata": {
        "id": "4D_d5lUJFMLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ilter method is faster and useful when there are more number of features. Wrapper method gives better performance while the embedded method lies in between the other two methods."
      ],
      "metadata": {
        "id": "TfDeie6JFMNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "42. How does correlation-based feature selection work?"
      ],
      "metadata": {
        "id": "Zq9nxFzIFMQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy."
      ],
      "metadata": {
        "id": "6XPWOP8IFMTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "43. How do you handle multicollinearity in feature selection?"
      ],
      "metadata": {
        "id": "bfFpfvZsFMWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "To address multicollinearity, techniques such as regularization or feature selection can be applied to select a subset of independent variables that are not highly correlated with each other. In this article, we will focus on the most common one – VIF (Variance Inflation Factors)."
      ],
      "metadata": {
        "id": "Ve7u7iY8GfYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "44. What are some common feature selection metrics?"
      ],
      "metadata": {
        "id": "T5wB_Kc3GfbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Class separability\n",
        "Error probability\n",
        "Inter-class distance\n",
        "Probabilistic distance\n",
        "Entropy\n",
        "Consistency-based feature selection\n",
        "Correlation-based feature selection"
      ],
      "metadata": {
        "id": "Gu9_a1YzGffB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "45. Give an example scenario where feature selection can be applied."
      ],
      "metadata": {
        "id": "WV-c-IqGGfjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Below are some real-life examples of feature selection: Mammographic image analysis. Criminal behavior modeling. Genomic data analysis."
      ],
      "metadata": {
        "id": "6tyxMUGWGxNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "46. What is data drift in machine learning?"
      ],
      "metadata": {
        "id": "nwEXWGUyGxQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data drift, also known as covariate shift, occurs when the distribution of the input data changes over time. For example, consider a machine learning model that was trained to predict the likelihood of a customer purchasing a product based on their age and income."
      ],
      "metadata": {
        "id": "f2jb3XsBGxS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "47. Why is data drift detection important?"
      ],
      "metadata": {
        "id": "EW7ekD4NGxWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data drift can have major consequences for ML models including malfunction and performance deterioration. This can be a major barrier for ML tools to generalize to a wide range of healthcare institutions, disease patterns and image acquisition technologies, especially as these factors change over time."
      ],
      "metadata": {
        "id": "jooO5M0wGfm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "48. Explain the difference between concept drift and feature drift."
      ],
      "metadata": {
        "id": "SNKepgCpGfqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Concept drift indicates there's been a change in the underlying relationships between features and outcomes: the probability of Y output given X input or P(Y|X)."
      ],
      "metadata": {
        "id": "0vRi-gayHUup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "49. What are some techniques used for detecting data drift?"
      ],
      "metadata": {
        "id": "9DbXNjrIHUxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Time distribution-based methods use statistical methods to calculate the difference between two probability distributions to detect drift. These methods include the Population Stability Index, KL Divergence, JS Divergence, KS Test, and the Wasserstein Metric."
      ],
      "metadata": {
        "id": "k0dFRStwHU0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "50. How can you handle data drift in a machine learning model?"
      ],
      "metadata": {
        "id": "DuE_7sBnHU3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Some strategies for addressing drift include continuously monitoring and evaluating the performance of a model, updating the model with new data, and using machine learning models that are more robust to drift."
      ],
      "metadata": {
        "id": "oruSw5T1HU6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "51. What is data leakage in machine learning?\n"
      ],
      "metadata": {
        "id": "H4EK5ovuHU9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n simple words, data leakage can be defined as: \"A scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. It causes high performance while training set, but perform poorly in deployment or production.\""
      ],
      "metadata": {
        "id": "Zt5qYdhwHVBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "52. Why is data leakage a concern?"
      ],
      "metadata": {
        "id": "g_i1fIi5HVFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A data leak is when information is exposed to unauthorized people due to internal errors. This is often caused by poor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation."
      ],
      "metadata": {
        "id": "vhSZClK-HVJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "53. Explain the difference between target leakage and train-test contamination."
      ],
      "metadata": {
        "id": "kgqxmO-wHVMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production."
      ],
      "metadata": {
        "id": "1VPfnjI4IPoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "54. How can you identify and prevent data leakage in a machine learning pipeline?"
      ],
      "metadata": {
        "id": "J4SC6ZzTIPrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data."
      ],
      "metadata": {
        "id": "Mdf8U71GIPuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "55. What are some common sources of data leakage?"
      ],
      "metadata": {
        "id": "QsTjdv8IIPxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8 Most Common Causes of Data Breach\n",
        "Weak and Stolen Credentials, a.k.a. Passwords. ...\n",
        "Back Doors, Application Vulnerabilities. ...\n",
        "Malware. ...\n",
        "Social Engineering. ...\n",
        "Too Many Permissions. ...\n",
        "Insider Threats. ...\n",
        "Physical Attacks. ...\n",
        "Improper Configuration, User Error."
      ],
      "metadata": {
        "id": "LUjBgz1xIP0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "56. Give\n",
        "\n",
        " an example scenario where data leakage can occur.\n"
      ],
      "metadata": {
        "id": "oT3Y2dLEJGfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data leakage occurs when sensitive data gets unintentionally exposed to the public in transit, at rest, or in use. Here are common examples: Data exposed in transit — Data transmitted via emails, API calls, chat rooms, and other communications."
      ],
      "metadata": {
        "id": "6shKb_7aJGh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "57. What is cross-validation in machine learning?"
      ],
      "metadata": {
        "id": "1QMlX3lEJGkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds"
      ],
      "metadata": {
        "id": "fYxjNyRbJGnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "58. Why is cross-validation important?"
      ],
      "metadata": {
        "id": "3Dko6m2CJGpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."
      ],
      "metadata": {
        "id": "LIm2DVy1JGs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation."
      ],
      "metadata": {
        "id": "61gkKG_8JGvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KFold devides the dataset into k folds. Where as Stratified ensures that each fold of dataset has the same proportion of observations with a given label."
      ],
      "metadata": {
        "id": "bS_Q-YZdJGyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "60. How do you interpret the cross-validation results?"
      ],
      "metadata": {
        "id": "RwtoImDqKCW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k-Fold Cross Validation:\n",
        "Take the group as a holdout or test data set.\n",
        "Take the remaining groups as a training data set.\n",
        "Fit a model on the training set and evaluate it on the test set.\n",
        "Retain the evaluation score and discard the model."
      ],
      "metadata": {
        "id": "kdirBl1yKCaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuIlhAy0JG2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}